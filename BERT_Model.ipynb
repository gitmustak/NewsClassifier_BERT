{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "# UPDATE THIS PATH to where your dataset is located\n",
        "DATASET_PATH = '/content/drive/MyDrive/Model epoch 3/News_Category_Dataset_v3.json'\n",
        "df = pd.read_json(DATASET_PATH, lines=True)\n",
        "\n",
        "print(f\"Total articles loaded: {len(df)}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Combine headline and short_description for better context\n",
        "df['text'] = df['headline'] + ' [SEP] ' + df['short_description']\n",
        "\n",
        "# Filter out categories with too few samples (keeps categories with 100+ samples)\n",
        "category_counts = df['category'].value_counts()\n",
        "print(f\"\\nOriginal number of categories: {len(category_counts)}\")\n",
        "top_categories = category_counts[category_counts >= 100].index\n",
        "df = df[df['category'].isin(top_categories)]\n",
        "\n",
        "print(f\"Dataset shape after filtering: {df.shape}\")\n",
        "print(f\"Number of categories (100+ samples): {df['category'].nunique()}\")\n",
        "print(f\"\\nTop 10 categories:\")\n",
        "print(df['category'].value_counts().head(10))\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['category'])\n",
        "\n",
        "# Save label encoder for later use\n",
        "label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
        "with open('label_mapping.json', 'w') as f:\n",
        "    json.dump(label_mapping, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Saved label_mapping.json with {len(label_mapping)} categories\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['text'].values,\n",
        "    df['label'].values,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "\n",
        "# Custom Dataset class\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "print(\"\\nLoading BERT model...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "num_labels = len(label_encoder.classes_)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded with {num_labels} output classes\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = NewsDataset(X_train, y_train, tokenizer)\n",
        "val_dataset = NewsDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32  # Increased for better GPU utilization\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "ACCUMULATION_STEPS = 2  # Gradient accumulation for effective batch size of 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Effective batch size (with accumulation): {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training function with gradient accumulation\n",
        "def train_epoch(model, data_loader, optimizer, scheduler, device, accumulation_steps=2):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for idx, batch in enumerate(tqdm(data_loader, desc=\"Training\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Normalize loss for accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item() * accumulation_steps)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights every accumulation_steps\n",
        "        if (idx + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_accuracy = 0\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 70)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        device,\n",
        "        ACCUMULATION_STEPS\n",
        "    )\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(model, val_loader, device)\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc.item())\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc.item())\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        print(f'‚úÖ Best model saved! Accuracy: {best_accuracy:.4f}')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Save the final model\n",
        "print(\"\\nSaving final model...\")\n",
        "model.save_pretrained('news_classifier_bert')\n",
        "tokenizer.save_pretrained('news_classifier_bert')\n",
        "\n",
        "print(\"‚úÖ Model saved to 'news_classifier_bert/' directory\")\n",
        "print(\"‚úÖ Label mapping saved to 'label_mapping.json'\")\n",
        "print(\"‚úÖ Best weights saved to 'best_model_state.bin'\")\n",
        "\n",
        "# Print training summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total epochs: {EPOCHS}\")\n",
        "print(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {history['val_acc'][-1]:.4f}\")\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Number of categories: {num_labels}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save training history\n",
        "with open('training_history.json', 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(\"\\n‚úÖ Training history saved to 'training_history.json'\")"
      ],
      "metadata": {
        "id": "U9SWP3TOtdo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybXwV2crouOB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import os\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "MODEL_PATH = '/content/drive/MyDrive/Model epoch 3/news_classifier_bert'\n",
        "DATASET_PATH = '/content/drive/MyDrive/Model epoch 3/News_Category_Dataset_v3.json'\n",
        "\n",
        "# ===== FIX: Load the dataset =====\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_json(DATASET_PATH, lines=True)  # lines=True because it's a JSON Lines file\n",
        "print(f\"‚úÖ Dataset loaded: {len(df)} articles\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Sample categories: {df['category'].unique()[:10]}\\n\")\n",
        "# ================================\n",
        "\n",
        "# Create label mapping\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['category'])\n",
        "label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "with open('label_mapping.json', 'w') as f:\n",
        "    json.dump(label_mapping, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Created label_mapping.json with {len(label_mapping)} categories\\n\")\n",
        "\n",
        "# Defining classifier\n",
        "class NewsClassifier:\n",
        "    def __init__(self, model_path, label_mapping_path='label_mapping.json'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        with open(label_mapping_path, 'r') as f:\n",
        "            self.label_mapping = json.load(f)\n",
        "            self.label_mapping = {int(k): v for k, v in self.label_mapping.items()}\n",
        "\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "        print(f\"Number of categories: {len(self.label_mapping)}\\n\")\n",
        "\n",
        "    def predict(self, text=None, headline=None, description=None):\n",
        "        if headline and description:\n",
        "            input_text = f\"{headline} [SEP] {description}\"\n",
        "        elif text:\n",
        "            input_text = text\n",
        "        else:\n",
        "            raise ValueError(\"Must provide either 'text' or both 'headline' and 'description'\")\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        top_probs, top_indices = torch.topk(probabilities, k=min(5, len(self.label_mapping)))\n",
        "        top_probs = top_probs.cpu().numpy()[0]\n",
        "        top_indices = top_indices.cpu().numpy()[0]\n",
        "\n",
        "        predicted_label = top_indices[0]\n",
        "        predicted_category = self.label_mapping[predicted_label]\n",
        "        confidence = top_probs[0]\n",
        "\n",
        "        top_predictions = [\n",
        "            {'category': self.label_mapping[idx], 'confidence': float(prob)}\n",
        "            for idx, prob in zip(top_indices, top_probs)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'predicted_category': predicted_category,\n",
        "            'confidence': float(confidence),\n",
        "            'top_predictions': top_predictions\n",
        "        }\n",
        "\n",
        "# Initializing classifier\n",
        "print(\"=\"*80)\n",
        "print(\"BERT NEWS CLASSIFIER - COMPREHENSIVE TEST SUITE\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "classifier = NewsClassifier(model_path=MODEL_PATH)\n",
        "\n",
        "# Sample Test Articles\n",
        "test_articles = [\n",
        "    {\n",
        "        \"headline\": \"Apple Unveils iPhone 16 with Revolutionary AI Chip\",\n",
        "        \"description\": \"Tech giant Apple announced its latest smartphone featuring a groundbreaking neural processing unit that promises 10x faster AI performance and week-long battery life.\",\n",
        "        \"expected\": \"TECH or BUSINESS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"LeBron James Becomes NBA's All-Time Leading Scorer\",\n",
        "        \"description\": \"The Los Angeles Lakers star broke Kareem Abdul-Jabbar's long-standing record with a fadeaway jumper in the third quarter against the Oklahoma City Thunder.\",\n",
        "        \"expected\": \"SPORTS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"New Study Links Mediterranean Diet to 30% Lower Dementia Risk\",\n",
        "        \"description\": \"Researchers at Harvard Medical School conducted a 20-year study showing that people who follow a Mediterranean diet rich in olive oil, fish, and vegetables have significantly lower rates of cognitive decline.\",\n",
        "        \"expected\": \"HEALTH or WELLNESS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Federal Reserve Cuts Interest Rates by 0.5% to Boost Economy\",\n",
        "        \"description\": \"In a surprise move, the Fed announced a larger-than-expected rate cut aimed at preventing a potential recession as inflation continues to moderate and unemployment rises.\",\n",
        "        \"expected\": \"BUSINESS or MONEY\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Taylor Swift's Eras Tour Breaks Records with $2 Billion in Ticket Sales\",\n",
        "        \"description\": \"The pop superstar's worldwide concert tour has become the highest-grossing music tour in history, surpassing previous records and boosting local economies in every city visited.\",\n",
        "        \"expected\": \"ENTERTAINMENT\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Scientists Discover Habitable Exoplanet Just 40 Light-Years Away\",\n",
        "        \"description\": \"Astronomers using the James Webb Space Telescope have identified a rocky planet in the habitable zone of its star with atmospheric conditions that could support liquid water and potentially life.\",\n",
        "        \"expected\": \"SCIENCE\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Supreme Court Rules on Landmark Environmental Protection Case\",\n",
        "        \"description\": \"In a 6-3 decision, the court upheld federal regulations limiting carbon emissions from power plants, marking a significant victory for environmental advocates and the Biden administration.\",\n",
        "        \"expected\": \"POLITICS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Viral TikTok Recipe for Cloud Bread Takes Internet by Storm\",\n",
        "        \"description\": \"A simple three-ingredient recipe for fluffy, Instagram-worthy bread has garnered over 100 million views, with home bakers worldwide attempting the trendy technique that requires just eggs, sugar, and cornstarch.\",\n",
        "        \"expected\": \"FOOD & DRINK or TASTE\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Paris Fashion Week Showcases Sustainable Luxury Collections\",\n",
        "        \"description\": \"Major fashion houses including Chanel, Dior, and Louis Vuitton presented eco-friendly haute couture lines featuring recycled materials and carbon-neutral production methods.\",\n",
        "        \"expected\": \"STYLE & BEAUTY or STYLE\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"New Parenting App Uses AI to Detect Signs of Postpartum Depression\",\n",
        "        \"description\": \"A Silicon Valley startup has developed an application that monitors new mothers' speech patterns, sleep, and activity levels to identify early warning signs of postpartum depression and connect them with mental health resources.\",\n",
        "        \"expected\": \"PARENTING\"\n",
        "    }\n",
        "]\n",
        "\n",
        "#Testing\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING MODEL ON 10 DIVERSE NEWS ARTICLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "correct_predictions = 0\n",
        "total_tests = len(test_articles)\n",
        "all_results = []  # Store results to avoid re-running predictions\n",
        "\n",
        "for i, article in enumerate(test_articles, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEST CASE #{i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"üì∞ Headline: {article['headline']}\")\n",
        "    print(f\"üìù Description: {article['description']}\")\n",
        "    print(f\"üéØ Expected Category: {article['expected']}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    result = classifier.predict(\n",
        "        headline=article['headline'],\n",
        "        description=article['description']\n",
        "    )\n",
        "    all_results.append(result)  # Store for later use\n",
        "\n",
        "    print(f\"\\n‚úÖ PREDICTED: {result['predicted_category']}\")\n",
        "    print(f\"üíØ CONFIDENCE: {result['confidence']:.2%}\")\n",
        "\n",
        "    expected_categories = article['expected'].split(' or ')\n",
        "    if result['predicted_category'] in expected_categories:\n",
        "        print(f\"‚úì CORRECT PREDICTION!\")\n",
        "        correct_predictions += 1\n",
        "    else:\n",
        "        print(f\"‚úó Prediction differs from expected\")\n",
        "\n",
        "    print(f\"\\nüìä TOP 5 PREDICTIONS:\")\n",
        "    for j, pred in enumerate(result['top_predictions'], 1):\n",
        "        bar_length = int(pred['confidence'] * 50)\n",
        "        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (50 - bar_length)\n",
        "\n",
        "        marker = \"‚úì\" if pred['category'] in expected_categories else \" \"\n",
        "        print(f\"  {marker} {j}. {pred['category']:<25} {bar} {pred['confidence']:.2%}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total test cases: {total_tests}\")\n",
        "print(f\"Correct predictions: {correct_predictions}\")\n",
        "print(f\"Accuracy: {(correct_predictions/total_tests)*100:.1f}%\")\n",
        "print(f\"Model categories available: {len(classifier.label_mapping)}\")\n",
        "print(f\"Device used: {classifier.device}\")\n",
        "\n",
        "# Average confidence using stored results (more efficient)\n",
        "avg_confidence = sum([result['confidence'] for result in all_results]) / len(all_results)\n",
        "print(f\"Average confidence: {avg_confidence:.2%}\")\n",
        "\n",
        "print(f\"\\nüìã Available categories in model:\")\n",
        "categories_list = list(classifier.label_mapping.values())\n",
        "for i in range(0, len(categories_list), 5):\n",
        "    print(\"  \" + \", \".join(categories_list[i:i+5]))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING COMPLETE! üéâ\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "dV8ar4LNzOZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # STEP 0: Find your files\n",
        "# print(\"Searching for dataset and model...\")\n",
        "# print(\"\\nFiles in Google Drive root:\")\n",
        "# !ls \"/content/drive/MyDrive/\" | head -20\n",
        "\n",
        "\n",
        "MODEL_PATH = '/content/drive/MyDrive/Model epoch 3/news_classifier_bert'\n",
        "DATASET_PATH = '/content/drive/MyDrive/Model epoch 3/News_Category_Dataset_v3.json'\n",
        "\n",
        "\n",
        "\n",
        "# Verify files exist\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f\"‚ùå Model not found at: {MODEL_PATH}\")\n",
        "    print(\"\\nSearching for model folder...\")\n",
        "    !find /content/drive/MyDrive -name \"news_classifier_bert\" -type d 2>/dev/null\n",
        "    raise FileNotFoundError(f\"Model not found. Please update MODEL_PATH\")\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    print(f\"‚ùå Dataset not found at: {DATASET_PATH}\")\n",
        "    print(\"\\nSearching for dataset...\")\n",
        "    !find /content/drive/MyDrive -name \"*.json\" -type f 2>/dev/null | grep -i news | head -5\n",
        "    raise FileNotFoundError(f\"Dataset not found. Please update DATASET_PATH\")\n",
        "\n",
        "print(f\"‚úÖ Model found at: {MODEL_PATH}\")\n",
        "print(f\"‚úÖ Dataset found at: {DATASET_PATH}\")\n",
        "\n",
        "# STEP 1: Load dataset and create label_mapping.json\n",
        "print(\"\\nLoading dataset...\")\n",
        "try:\n",
        "    df = pd.read_json(DATASET_PATH, lines=True)\n",
        "    print(f\"‚úÖ Loaded {len(df)} articles\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Filter categories\n",
        "category_counts = df['category'].value_counts()\n",
        "print(f\"\\nTotal unique categories: {len(category_counts)}\")\n",
        "top_categories = category_counts[category_counts >= 100].index\n",
        "df = df[df['category'].isin(top_categories)]\n",
        "print(f\"Filtered to {len(top_categories)} categories with 100+ samples\")\n",
        "\n",
        "# Create label mapping\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['category'])\n",
        "label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "with open('label_mapping.json', 'w') as f:\n",
        "    json.dump(label_mapping, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Created label_mapping.json with {len(label_mapping)} categories\\n\")\n",
        "\n",
        "# STEP 2: Define classifier\n",
        "class NewsClassifier:\n",
        "    def __init__(self, model_path, label_mapping_path='label_mapping.json'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        with open(label_mapping_path, 'r') as f:\n",
        "            self.label_mapping = json.load(f)\n",
        "            self.label_mapping = {int(k): v for k, v in self.label_mapping.items()}\n",
        "\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "        print(f\"Number of categories: {len(self.label_mapping)}\\n\")\n",
        "\n",
        "    def predict(self, text=None, headline=None, description=None):\n",
        "        if headline and description:\n",
        "            input_text = f\"{headline} [SEP] {description}\"\n",
        "        elif text:\n",
        "            input_text = text\n",
        "        else:\n",
        "            raise ValueError(\"Must provide either 'text' or both 'headline' and 'description'\")\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        top_probs, top_indices = torch.topk(probabilities, k=min(5, len(self.label_mapping)))\n",
        "        top_probs = top_probs.cpu().numpy()[0]\n",
        "        top_indices = top_indices.cpu().numpy()[0]\n",
        "\n",
        "        predicted_label = top_indices[0]\n",
        "        predicted_category = self.label_mapping[predicted_label]\n",
        "        confidence = top_probs[0]\n",
        "\n",
        "        top_predictions = [\n",
        "            {'category': self.label_mapping[idx], 'confidence': float(prob)}\n",
        "            for idx, prob in zip(top_indices, top_probs)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'predicted_category': predicted_category,\n",
        "            'confidence': float(confidence),\n",
        "            'top_predictions': top_predictions\n",
        "        }\n",
        "\n",
        "# STEP 3: Initialize classifier\n",
        "print(\"=\"*80)\n",
        "print(\"BERT NEWS CLASSIFIER - COMPREHENSIVE TEST SUITE\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "classifier = NewsClassifier(model_path=MODEL_PATH)\n",
        "\n",
        "# STEP 4: Diverse test articles\n",
        "test_articles = [\n",
        "    {\n",
        "        \"headline\": \"Apple Unveils iPhone 16 with Revolutionary AI Chip\",\n",
        "        \"description\": \"Tech giant Apple announced its latest smartphone featuring a groundbreaking neural processing unit that promises 10x faster AI performance.\",\n",
        "        \"expected\": \"TECH\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"LeBron James Becomes NBA's All-Time Leading Scorer\",\n",
        "        \"description\": \"The Los Angeles Lakers star broke Kareem Abdul-Jabbar's long-standing record with a fadeaway jumper in the third quarter.\",\n",
        "        \"expected\": \"SPORTS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Mediterranean Diet Linked to 30% Lower Dementia Risk\",\n",
        "        \"description\": \"Harvard researchers conducted a 20-year study showing people following a Mediterranean diet have significantly lower rates of cognitive decline.\",\n",
        "        \"expected\": \"HEALTHY LIVING\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Federal Reserve Cuts Interest Rates by 0.5%\",\n",
        "        \"description\": \"The Fed announced a larger-than-expected rate cut aimed at preventing recession as inflation moderates and unemployment rises.\",\n",
        "        \"expected\": \"BUSINESS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Taylor Swift's Eras Tour Breaks Records with $2 Billion\",\n",
        "        \"description\": \"The pop superstar's worldwide concert tour has become the highest-grossing music tour in history, boosting local economies everywhere.\",\n",
        "        \"expected\": \"ENTERTAINMENT\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Scientists Discover Habitable Exoplanet 40 Light-Years Away\",\n",
        "        \"description\": \"Astronomers using James Webb Telescope identified a rocky planet with atmospheric conditions that could support liquid water.\",\n",
        "        \"expected\": \"SCIENCE\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Supreme Court Rules on Environmental Protection Case\",\n",
        "        \"description\": \"In a 6-3 decision, the court upheld federal regulations limiting carbon emissions, marking a victory for environmental advocates.\",\n",
        "        \"expected\": \"POLITICS\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Viral TikTok Cloud Bread Recipe Takes Internet by Storm\",\n",
        "        \"description\": \"A simple three-ingredient recipe has garnered 100 million views, with home bakers worldwide attempting the trendy fluffy technique.\",\n",
        "        \"expected\": \"TASTE\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"Paris Fashion Week Showcases Sustainable Luxury\",\n",
        "        \"description\": \"Chanel, Dior, and Louis Vuitton presented eco-friendly haute couture featuring recycled materials and carbon-neutral production.\",\n",
        "        \"expected\": \"STYLE & BEAUTY\"\n",
        "    },\n",
        "    {\n",
        "        \"headline\": \"AI App Detects Postpartum Depression in New Mothers\",\n",
        "        \"description\": \"A startup developed an application that monitors speech patterns and sleep to identify early warning signs of postpartum depression.\",\n",
        "        \"expected\": \"PARENTING\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# STEP 5: Run tests\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TESTING 10 DIVERSE ARTICLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, article in enumerate(test_articles, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEST #{i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"üì∞ {article['headline']}\")\n",
        "    print(f\"üìù {article['description'][:80]}...\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    result = classifier.predict(headline=article['headline'], description=article['description'])\n",
        "\n",
        "    print(f\"\\nüéØ PREDICTED: {result['predicted_category']} ({result['confidence']:.2%})\")\n",
        "    print(f\"\\nTop 5:\")\n",
        "    for j, pred in enumerate(result['top_predictions'], 1):\n",
        "        bar = \"‚ñà\" * int(pred['confidence'] * 30)\n",
        "        print(f\"  {j}. {pred['category']:<25} {bar} {pred['confidence']:.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLETE! üéâ\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "uUdqYIOp2Efl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}